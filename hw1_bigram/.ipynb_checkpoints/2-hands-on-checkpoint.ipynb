{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение корпуса (=файла)\n",
    "corpus = None\n",
    "corpus_name = 'data.txt'\n",
    "with open(corpus_name) as fin:\n",
    "    corpus = fin.read()\n",
    "print('Corpus length (in symbols) = %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем текст на слова. Слово -- последовательность символов, разделенных чем-то кроме букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ##\n",
    "print('Corpus length (in words) = %d' % len(words))\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать с знаками препинания?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ##\n",
    "print('Corpus length (in words) = %d' % len(words))\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём все слова и посчитаем частоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ##\n",
    "word_counts = Counter(words)\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать с регистром?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lower = ##\n",
    "word_lower_counts = Counter(words_lower)\n",
    "\n",
    "print(word_lower_counts.most_common(10))\n",
    "print('Different words without lowering case:  %d' % len(word_counts))\n",
    "print('Different words with lowering case: %d' % len(word_lower_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка разных полезных данных\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conll2000.ensure_loaded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?conll2000 # можно посмотреть справку по любому объекту Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встроенный токенизатор!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встроенные теггеры!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = conll2000.tagged_sents()[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "combined_bigram_tagger = nltk.BigramTagger(train_sents, backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разметить предложение. Что пошло не так?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tagger.tag(\"Mary had a little lamb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправляемся!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция, которая считает точность.\n",
    "def accuracy(test_sents, postagger):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for sent in test_sents:\n",
    "        length += len(sent)\n",
    "        sent, real_tags = zip(*sent)  # что тут произошло?\n",
    "        my_tags = ##\n",
    "        for i in range(len(my_tags)):\n",
    "            ##\n",
    "    return ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = conll2000.tagged_sents()[8000:]\n",
    "accuracy(test_sents, combined_bigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простейший POS-теггер\n",
    "Для каждого слова определим какими тегами оно бывает отмечено в обучающем корпусе, а для предсказания выбираем наиболее частотный тег:\n",
    "$$\n",
    "tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(tag_i \\mid w).\n",
    "$$\n",
    "NB! Если какое-то слово в обучающем корпусе мы не встретили, то тег на нем никаким образом уже поставить не сможем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Нормализатор для получения распределения вероятностей из частот\n",
    "class BaseNormalizer:\n",
    "    def normalize(self, counter):\n",
    "        ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Модель теггера\n",
    "# Здесь будут храниться слова и вероятности их тегов, например:\n",
    "# { 'content': Counter({'JJ': 0.3, 'NN': 0.7})}\n",
    "class WordTagModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # Для каждого слова добавим Counter с частотами тегов\n",
    "        # Не забудем нормализовать частоты, чтобы получить вероятности\n",
    "        ##\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Реализуем теггер наподобие nltk-шных\n",
    "class SimplePOSTagger:\n",
    "    # Инициализировать теггер будем моделью word_tag_model, реализация которой выше\n",
    "    def __init__(self, word_tag_model):\n",
    "        self.wts = word_tag_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        tags = []\n",
    "        # sent - предложение в таком же виде, как для теггеров NLTK, то есть список слов (list).\n",
    "        ##\n",
    "        # Что делать со словами, которых нет в модели?\n",
    "        ##\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем\n",
    "wtm = WordTagModel(train_sents)\n",
    "postagger = SimplePOSTagger(wtm)\n",
    "print(accuracy(test_sents, postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram tagger\n",
    "Для каждого слова будем выбирать наиболее вероятный тег, учитывая общую вероятность самого тега. Тут можно будет добавить сглаживание для слов, которых в модели не было.\n",
    "$$\n",
    "     tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(w \\mid tag_i)P(tag_i)\n",
    "$$\n",
    "Для этого нам понадобятся новые классы:\n",
    "\n",
    "**EmissionModel**, хранящий для каждого тега вероятности быть присвоенным тому или иному слову.\n",
    "\n",
    "**TransitionModel**, отвечающий за вероятность $P(tag_i)$.\n",
    "\n",
    "**UnigramPOSTagger**, сопоставляющий последовательности слов последовательность тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # self.model будет иметь вид \n",
    "        # defaultdict({'tag_1': Counter({'word_1': 0.3, 'word_2': 0.7}), 'tag_2': Counter({'word_1': 0.6, 'word_3': 0.3 ...})})\n",
    "        ##\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        # Для каждого тега добавим одинаковую вероятность быть приписанным любому слову, которого нет в модели\n",
    "        ##\n",
    "        \n",
    "    def tags(self):\n",
    "        # Добавим возможность возвращать все теги, которые есть в модели\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        # Все слова для данного тега\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        # Самое интересное - вероятность P(word|tag)\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionModel:\n",
    "    def __init__(self, tag_seqs, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        # Это модель будет хранить вероятности P(tag): Counter({'tag_1': 0.34, 'tag_2': 0.1 ...})\n",
    "        self.model = Counter()\n",
    "        ##\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, tag):\n",
    "        return self.model[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramPOSTagger:\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # Для списка слов возвращаем список пар (слово, тег)\n",
    "        # Для каждого слова проходимся по всем тегам\n",
    "        # И выбираем максимум по формуле\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            ##\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Проверяем!\n",
    "em = EmissionModel(train_sents)\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents])\n",
    "unigram_postagger = UnigramPOSTagger(em, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(test_sents, unigram_postagger))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
