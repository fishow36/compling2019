{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in symbols) = 1021\n"
     ]
    }
   ],
   "source": [
    "# Чтение корпуса (=файла)\n",
    "corpus = None\n",
    "corpus_name = 'data.txt'\n",
    "with open(corpus_name) as fin:\n",
    "    corpus = fin.read()\n",
    "print('Corpus length (in symbols) = %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем текст на слова. Слово -- последовательность символов, разделенных чем-то кроме букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in words) = 110\n",
      "['Тронный', 'зал', 'во', 'дворце', 'короля', 'Лира.', 'Входят', 'Кент,', 'Глостер', 'и', 'Эдмонд.', 'Кент', 'Я', 'думал,', 'что', 'герцог', 'Альбанский', 'нравится', 'королю', 'больше', 'герцога', 'Корнуэльского.', 'Глостер', 'Так', 'нам', 'всегда', 'казалось.', 'Но', 'теперь,', 'перед', 'разделом', 'королевства,', 'стало', 'неясно,', 'кого', 'он', 'любит', 'больше.', 'Части', 'так', 'выравнены,', 'что', 'при', 'самом', 'внимательном', 'разборе', 'нельзя', 'сказать,', 'какая', 'лучше.']\n"
     ]
    }
   ],
   "source": [
    "words = corpus.split()\n",
    "print('Corpus length (in words) = %d' % len(words))\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать с знаками препинания?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (in words) = 111\n",
      "['Тронный', 'зал', 'во', 'дворце', 'короля', 'Лира', 'Входят', 'Кент', 'Глостер', 'и', 'Эдмонд', 'Кент', 'Я', 'думал', 'что', 'герцог', 'Альбанский', 'нравится', 'королю', 'больше', 'герцога', 'Корнуэльского', 'Глостер', 'Так', 'нам', 'всегда', 'казалось', 'Но', 'теперь', 'перед', 'разделом', 'королевства', 'стало', 'неясно', 'кого', 'он', 'любит', 'больше', 'Части', 'так', 'выравнены', 'что', 'при', 'самом', 'внимательном', 'разборе', 'нельзя', 'сказать', 'какая', 'лучше']\n"
     ]
    }
   ],
   "source": [
    "words = re.split('\\W+', corpus)\n",
    "print('Corpus length (in words) = %d' % len(words))\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём все слова и посчитаем частоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Кент', 5), ('Глостер', 4), ('Я', 4), ('в', 4), ('что', 3), ('и', 2), ('больше', 2), ('так', 2), ('меня', 2), ('Тронный', 1), ('зал', 1), ('во', 1), ('дворце', 1), ('короля', 1), ('Лира', 1), ('Входят', 1), ('Эдмонд', 1), ('думал', 1), ('герцог', 1), ('Альбанский', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = re.findall('\\w+', corpus)\n",
    "word_counts = Counter(words)\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать с регистром?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('кент', 5), ('глостер', 4), ('я', 4), ('в', 4), ('что', 3), ('так', 3), ('и', 2), ('больше', 2), ('меня', 2), ('тронный', 1)]\n",
      "Different words without lowering case:  91\n",
      "Different words with lowering case: 90\n"
     ]
    }
   ],
   "source": [
    "words_lower = re.findall('\\w+', corpus.lower())\n",
    "word_lower_counts = Counter(words_lower)\n",
    "\n",
    "print(word_lower_counts.most_common(10))\n",
    "print('Different words without lowering case:  %d' % len(word_counts))\n",
    "print('Different words with lowering case: %d' % len(word_lower_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e13dec19f506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Загрузка разных полезных данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Загрузка разных полезных данных\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conll2000.ensure_loaded()\n",
    "#штуковина нужна чтобы наверняка скачать корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?conll2000 # можно посмотреть справку по любому объекту Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встроенный токенизатор!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тронный',\n",
       " 'зал',\n",
       " 'во',\n",
       " 'дворце',\n",
       " 'короля',\n",
       " 'Лира',\n",
       " '.',\n",
       " 'Входят',\n",
       " 'Кент',\n",
       " ',',\n",
       " 'Глостер',\n",
       " 'и',\n",
       " 'Эдмонд',\n",
       " '.',\n",
       " 'Кент',\n",
       " 'Я',\n",
       " 'думал',\n",
       " ',',\n",
       " 'что',\n",
       " 'герцог',\n",
       " 'Альбанский',\n",
       " 'нравится',\n",
       " 'королю',\n",
       " 'больше',\n",
       " 'герцога',\n",
       " 'Корнуэльского',\n",
       " '.',\n",
       " 'Глостер',\n",
       " 'Так',\n",
       " 'нам',\n",
       " 'всегда',\n",
       " 'казалось',\n",
       " '.',\n",
       " 'Но',\n",
       " 'теперь',\n",
       " ',',\n",
       " 'перед',\n",
       " 'разделом',\n",
       " 'королевства',\n",
       " ',',\n",
       " 'стало',\n",
       " 'неясно',\n",
       " ',',\n",
       " 'кого',\n",
       " 'он',\n",
       " 'любит',\n",
       " 'больше',\n",
       " '.',\n",
       " 'Части',\n",
       " 'так',\n",
       " 'выравнены',\n",
       " ',',\n",
       " 'что',\n",
       " 'при',\n",
       " 'самом',\n",
       " 'внимательном',\n",
       " 'разборе',\n",
       " 'нельзя',\n",
       " 'сказать',\n",
       " ',',\n",
       " 'какая',\n",
       " 'лучше',\n",
       " '.',\n",
       " 'Кент',\n",
       " 'Это',\n",
       " 'ваш',\n",
       " 'сын',\n",
       " ',',\n",
       " 'милорд',\n",
       " '?',\n",
       " 'Глостер',\n",
       " 'Я',\n",
       " 'причастен',\n",
       " ',',\n",
       " 'сэр',\n",
       " ',',\n",
       " 'к',\n",
       " 'его',\n",
       " 'рождению',\n",
       " '.',\n",
       " 'Я',\n",
       " 'так',\n",
       " 'часто',\n",
       " 'краснел',\n",
       " ',',\n",
       " 'признаваясь',\n",
       " 'в',\n",
       " 'этом',\n",
       " ',',\n",
       " 'что',\n",
       " 'постепенно',\n",
       " 'перестал',\n",
       " 'смущаться',\n",
       " '.',\n",
       " 'Кент',\n",
       " 'Я',\n",
       " 'не',\n",
       " 'понимаю',\n",
       " 'вас',\n",
       " '.',\n",
       " 'Глостер',\n",
       " 'Зато',\n",
       " 'мать',\n",
       " 'этого',\n",
       " 'молодца',\n",
       " 'поняла',\n",
       " 'меня',\n",
       " 'с',\n",
       " 'первого',\n",
       " 'взгляда',\n",
       " 'и',\n",
       " 'получила',\n",
       " 'сына',\n",
       " 'в',\n",
       " 'люльку',\n",
       " 'раньше',\n",
       " ',',\n",
       " 'чем',\n",
       " 'мужа',\n",
       " 'в',\n",
       " 'дом',\n",
       " '.',\n",
       " 'Вы',\n",
       " 'меня',\n",
       " 'осуждаете',\n",
       " '?',\n",
       " 'Кент',\n",
       " 'Нет',\n",
       " ',',\n",
       " 'если',\n",
       " 'в',\n",
       " 'итоге',\n",
       " 'получился',\n",
       " 'такой',\n",
       " 'бравый',\n",
       " 'малый',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встроенные теггеры!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = conll2000.tagged_sents()[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "combined_bigram_tagger = nltk.BigramTagger(train_sents, backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разметить предложение. Что пошло не так?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('r', 'NN'),\n",
       " ('y', None),\n",
       " (' ', None),\n",
       " ('h', None),\n",
       " ('a', 'DT'),\n",
       " ('d', None),\n",
       " (' ', None),\n",
       " ('a', 'DT'),\n",
       " (' ', None),\n",
       " ('l', None),\n",
       " ('i', None),\n",
       " ('t', None),\n",
       " ('t', None),\n",
       " ('l', None),\n",
       " ('e', None),\n",
       " (' ', None),\n",
       " ('l', None),\n",
       " ('a', 'DT'),\n",
       " ('m', None),\n",
       " ('b', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.tag(\"Mary had a little lamb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправляемся!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('lamb.', None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger.tag(\"Mary had a little lamb.\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция, которая считает точность.\n",
    "def accuracy(test_sents, postagger):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for sent in test_sents:\n",
    "        length += len(sent)\n",
    "        sent, real_tags = zip(*sent)  # что тут произошло?\n",
    "        my_tags = postagger.tag(sent)\n",
    "        for i in range(len(my_tags)):\n",
    "            if my_tags[i][1] != real_tags[i]:\n",
    "                errors += 1\n",
    "    return 1 - errors / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906919108959396"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents = conll2000.tagged_sents()[8000:]\n",
    "accuracy(test_sents, combined_bigram_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простейший POS-теггер\n",
    "Для каждого слова определим какими тегами оно бывает отмечено в обучающем корпусе, а для предсказания выбираем наиболее частотный тег:\n",
    "$$\n",
    "tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(tag_i \\mid w).\n",
    "$$\n",
    "NB! Если какое-то слово в обучающем корпусе мы не встретили, то тег на нем никаким образом уже поставить не сможем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Нормализатор для получения распределения вероятностей из частот\n",
    "class BaseNormalizer:\n",
    "    def normalize(self, counter):\n",
    "        sum_ = sum(counter.values())\n",
    "        for token in counter:\n",
    "            counter[token] /= sum_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Модель теггера\n",
    "# Здесь будут храниться слова и вероятности их тегов, например:\n",
    "# { 'content': Counter({'JJ': 0.3, 'NN': 0.7})}\n",
    "class WordTagModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # Для каждого слова добавим Counter с частотами тегов\n",
    "        for sent in tagged_sents:\n",
    "            for word, tag in sent:\n",
    "                self.model[word][tag] += 1\n",
    "        # Не забудем нормализовать частоты, чтобы получить вероятности\n",
    "        for word in self.model:\n",
    "            self.normalizer.normalize(self.model[word])\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Реализуем теггер наподобие nltk-шных\n",
    "class SimplePOSTagger:\n",
    "    # Инициализировать теггер будем моделью word_tag_model, реализация которой выше\n",
    "    def __init__(self, word_tag_model):\n",
    "        self.wts = word_tag_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        tags = []\n",
    "\n",
    "        # sent - предложение в таком же виде, как для теггеров NLTK, то есть список слов (list).\n",
    "        for word in sent:\n",
    "            if word in self.wts.model:\n",
    "                tags.append(self.wts.model[word].most_common(1)[0][0])\n",
    "            # Что делать со словами, которых нет в модели?\n",
    "            else:\n",
    "                tags.append('UNK')\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895637013342555\n"
     ]
    }
   ],
   "source": [
    "# Проверяем\n",
    "wtm = WordTagModel(train_sents)\n",
    "postagger = SimplePOSTagger(wtm)\n",
    "print(accuracy(test_sents, postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram tagger\n",
    "Для каждого слова будем выбирать наиболее вероятный тег, учитывая общую вероятность самого тега. Тут можно будет добавить сглаживание для слов, которых в модели не было.\n",
    "$$\n",
    "     tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(w \\mid tag_i)P(tag_i)\n",
    "$$\n",
    "Для этого нам понадобятся новые классы:\n",
    "\n",
    "**EmissionModel**, хранящий для каждого тега вероятности быть присвоенным тому или иному слову.\n",
    "\n",
    "**TransitionModel**, отвечающий за вероятность $P(tag_i)$.\n",
    "\n",
    "**UnigramPOSTagger**, сопоставляющий последовательности слов последовательность тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # self.model будет иметь вид \n",
    "        # defaultdict({'tag_1': Counter({'word_1': 0.3, 'word_2': 0.7}), 'tag_2': Counter({'word_1': 0.6, 'word_3': 0.3 ...})})\n",
    "        for sent in tagged_sents:\n",
    "            for word, tag in sent:\n",
    "                self.model[tag][word] += 1\n",
    "        self.add_unk_token()\n",
    "        for tag in self.model:\n",
    "            self.normalizer.normalize(self.model[tag])\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        # Для каждого тега добавим одинаковую вероятность быть приписанным любому слову, которого нет в модели\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.1\n",
    "        \n",
    "    def tags(self):\n",
    "        # Добавим возможность возвращать все теги, которые есть в модели\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        # Все слова для данного тега\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        # Самое интересное - вероятность P(word|tag)\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionModel:\n",
    "    def __init__(self, tag_seqs, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        # Это модель будет хранить вероятности P(tag): Counter({'tag_1': 0.34, 'tag_2': 0.1 ...})\n",
    "        self.model = Counter()\n",
    "        for sent in tag_seqs:\n",
    "            self.model.update(sent)\n",
    "        self.normalizer.normalize(self.model)\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, tag):\n",
    "        return self.model[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramPOSTagger:\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # Для списка слов возвращаем список пар (слово, тег)\n",
    "        # Для каждого слова проходимся по всем тегам\n",
    "        # И выбираем максимум по формуле\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            max_prob = 0\n",
    "            best_tag = 'UNK'\n",
    "            for t in self.tm.tags():\n",
    "                prob = self.em(word, t) * self.tm(t)\n",
    "                if prob > max_prob:\n",
    "                    max_prob, best_tag = prob, t\n",
    "            tags.append(best_tag)\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Проверяем!\n",
    "em = EmissionModel(train_sents)\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents])\n",
    "unigram_postagger = UnigramPOSTagger(em, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents = conll2000.tagged_sents()[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072937379326245\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(test_sents, unigram_postagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
