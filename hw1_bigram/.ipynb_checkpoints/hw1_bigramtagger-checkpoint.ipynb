{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from conllu import parse\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'en_ewt-ud-train.conllu'\n",
    "with open (train, encoding = 'utf-8') as f:\n",
    "    data_train = f.read()\n",
    "train_sent = parse(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'en_ewt-ud-test.conllu'\n",
    "with open (test, encoding = 'utf-8') as f:\n",
    "    data_test = f.read()\n",
    "test_sent = parse(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = []\n",
    "for sent in train_sent:\n",
    "    sent_list = []\n",
    "    for token in sent:\n",
    "        sent_list.append(tuple((token['form'], token['upostag'])))\n",
    "    train_sents.append(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "for sent in test_sent:\n",
    "    sent_list = []\n",
    "    for token in sent:\n",
    "        sent_list.append(tuple((token['form'], token['upostag'])))\n",
    "    test_sents.append(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramTagger:\n",
    "    def __init__(self, train_sents):\n",
    "        self.alpha = 1 ## чтобы не делить на ноль. Может, не один надо добавлять?..\n",
    "        tags = [pair[1] for sent in train_sents for pair in sent]\n",
    "        self.tagset = list(set(tags))\n",
    "        self.tags = {tag:i for i, tag in enumerate(['<b>'] + self.tagset + ['<e>'] + ['UNK'])} ##переводит теги в индексы в матрице\n",
    "        words = [pair[0] for sent in train_sents for pair in sent]\n",
    "        vocabulary = list(set(words))\n",
    "        self.vocab = {word:i for i, word in enumerate(vocabulary)} ##переводит слова в индексы в матрице\n",
    "        self.emission = np.zeros((len(self.tags), (len(self.vocab)))) ##[tag][worrd]\n",
    "        self.transition = np.zeros((len(self.tags), len(self.tags))) ## [tag_i-1][tag_i]\n",
    "        self.update_trans(train_sents)\n",
    "        self.update_emis(train_sents)\n",
    "        self.emission = self.normalize(self.emission)\n",
    "        self.transition = self.normalize(self.transition)\n",
    "        #self.classify_sent(test_sents)\n",
    "        \n",
    "            \n",
    "        \n",
    "    def update_trans(self, train_sents):\n",
    "        for sent in train_sents:\n",
    "            tags = [pair[1] for pair in sent]\n",
    "            tags_seq = [tag for tag in (['<b>'] + tags + ['<e>'] + ['UNK'])]\n",
    "            for i in range(1, len(tags_seq)):\n",
    "                t_i = self.tags[tags_seq[i]]\n",
    "                t_imin1 = self.tags[tags_seq[i-1]]\n",
    "                self.transition[t_imin1][t_i] +=1\n",
    "        self.transition += self.alpha\n",
    "                \n",
    "    def update_emis(self, train_sents):\n",
    "        for sent in train_sents:\n",
    "            for pair in sent:\n",
    "                word = self.vocab[pair[0]]\n",
    "                tag = self.tags[pair[1]]\n",
    "                self.emission[tag][word] +=1\n",
    "        self.emission += self.alpha\n",
    "        \n",
    "    def normalize(self, trellis):\n",
    "        trellis = [[trellis[t][w]/np.sum(tag)  for w, word in enumerate(tag)] for t, tag in enumerate(trellis)]\n",
    "        return trellis\n",
    "    \n",
    "    def classify_ex(self, word_i, tag_imin1): #на вход слово i ('word') и tag i-1\n",
    "        if word_i not in self.vocab:\n",
    "            tag = 'UNK'\n",
    "        else:\n",
    "            word_ind = self.vocab[word_i]\n",
    "            max_prob = 0\n",
    "            for t in self.tagset:\n",
    "                tag_i_ind = self.tags[t]\n",
    "                tag_imin1_ind = self.tags[tag_imin1]\n",
    "                prob = self.emission[tag_i_ind][word_ind] * self.transition[tag_imin1_ind][tag_i_ind] ## word i tag i, tag i tag i-1\n",
    "                if prob > max_prob:\n",
    "                    max_prob, tag = prob, t\n",
    "        return tag\n",
    "    \n",
    "    def classify_sent(self, sent): #на вход список слов ['word1', 'word2'...]\n",
    "        tags_seq = ['<b>']\n",
    "        for i in range(len(sent)):\n",
    "            tag = self.classify_ex(sent[i],tags_seq[i]) #одинаковые индексы! потому что тэги начинаются с <b> а предложение нет\n",
    "            tags_seq.append(tag)\n",
    "        tags_seq.remove('<b>')\n",
    "        return list(zip(sent, tags_seq))\n",
    "    \n",
    "    #def classify(self, test_sents):\n",
    "    #    all_tags = []\n",
    "    #    for sent in test_sents:\n",
    "    #        sent_tags = self.classify_sent(sent)\n",
    "    #        all_tags.append(sent_tags)\n",
    "    #    return all_tags\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_sents, postagger):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for sent in test_sents:\n",
    "        length += len(sent)\n",
    "        sent, real_tags = zip(*sent)  # что тут произошло?\n",
    "        my_tags = postagger.classify_sent(sent)\n",
    "        for i in range(len(my_tags)):\n",
    "            if my_tags[i][1] != real_tags[i]:\n",
    "                errors += 1\n",
    "    return 1 - errors / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ud = accuracy(test_sents, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nltk = conll2000.tagged_sents()[:8000]\n",
    "test_nltk = conll2000.tagged_sents()[8000:]\n",
    "\n",
    "model_nltk = BigramTagger(train_nltk)\n",
    "acc_nltk = accuracy(test_nltk, model_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели, обученной и протестированной на корпусе UD: 0.8522728938892609\n",
      "Точность модели, обученной и протестированной на корпусе  conll2000: 0.8614593239387913\n"
     ]
    }
   ],
   "source": [
    "print('Точность модели, обученной и протестированной на корпусе UD: %s' % acc_ud)\n",
    "print('Точность модели, обученной и протестированной на корпусе  conll2000: %s' % acc_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
